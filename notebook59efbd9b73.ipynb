{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"qMwfXp7Bi2Fk","outputId":"80b0f331-363a-4127-c10f-e0e55aa56ba7","scrolled":true,"trusted":true},"outputs":[],"source":["!pip install -r /kaggle/input/aaaaaa/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ETPqkp9laHU","outputId":"ca8e5277-2838-49c5-a721-58efdedec1f7","scrolled":true,"trusted":true},"outputs":[],"source":["!pip install captcha"]},{"cell_type":"markdown","metadata":{"id":"vpj6qLvPk_le"},"source":["Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vo0hIvGKk-5T","trusted":true},"outputs":[],"source":["from torchtext.vocab import build_vocab_from_iterator\n","from collections import Counter\n","from rich import console\n","\n","console = console.Console()\n","\n","\n","def get_default_vocab():\n","    cab_a = r'abcdefghijklmnopqrstuvwxyz'\n","    cab_A = r'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n","    cab_0 = r'0123456789'\n","    cab_t = cab_a + cab_0\n","\n","    cab = [str(word) for word in cab_t]\n","    cab = build_vocab_from_iterator(cab, specials=['<BOS>', '<EOS>'], max_tokens=len(cab) + 2)\n","    return cab"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pwd\n","! rm -r ./captcha.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d-pdeCEDlFBf","outputId":"751ab2bc-fb05-49ef-8ba4-825c26c5dbc4","scrolled":true,"trusted":true},"outputs":[],"source":["import os.path\n","import pandas as pd\n","from captcha import image\n","import random\n","\n","dir_to_save = r'./gener_captchas'\n","cab_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n","            'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u',\n","            'v', 'w', 'x', 'y', 'z',\n","            # 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U',\n","            # 'V',\n","            # 'W', 'X', 'Y', 'Z'\n","            ]\n","\n","width, height = 160, 60\n","generator = image.ImageCaptcha(width=width, height=height)\n","file_paths = []\n","lables = []\n","if not os.path.exists(dir_to_save):\n","    os.mkdir(dir_to_save)\n","for i in range(20000):\n","\n","    random_str = ''.join([random.choice(cab_list) for j in range(5)])\n","    if random_str in lables:\n","        continue\n","    img = generator.generate_image(random_str)\n","    # img =generator.create_noise_curve(img, '#000000')\n","    file_path = os.path.join(dir_to_save, f'{random_str}_{str(i)}' + '.jpg')\n","#     print(file_path)\n","    file_paths.append(file_path)\n","    lables.append(random_str)\n","    img.save(file_path)\n","dataframe = pd.DataFrame(list(zip(file_paths, lables)), columns=['path', 'lable'])\n","print('done')\n","dataframe.to_csv(r'./captcha.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUS7KJM3l8jr","trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","\n","class MyDataSet(Dataset):\n","    def __init__(self, csv_path, cab):\n","        super(MyDataSet, self).__init__()\n","        self.data = pd.read_csv(csv_path)\n","        self.cab = cab\n","        self.stoi = self.cab.get_stoi()\n","        self.itos = self.cab.get_itos()\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        dicts = self.data.iloc[idx].to_dict()\n","        lable = dicts['lable']\n","        lable = [word for word in str(lable)]\n","        lable.insert(0, '<BOS>')\n","        lable.append('<EOS>')\n","        lable = map(lambda x: self.stoi[x], lable)\n","        return {'path': dicts['path'], 'lable': list(lable)}"]},{"cell_type":"markdown","metadata":{},"source":["F_Dateset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","\n","\n","class MyDataSet(Dataset):\n","    def __init__(self, csv_path, cab):\n","        super(MyDataSet, self).__init__()\n","        self.data = pd.read_csv(csv_path)\n","        self.cab = cab\n","        self.stoi = self.cab.get_stoi()\n","        self.itos = self.cab.get_itos()\n","        self.items = []\n","        for idx in range(len(self.data)):\n","#             if idx % 100==0:\n","#                 print(idx)\n","            dicts = self.data.iloc[idx].to_dict()\n","            lable = dicts['lable']\n","            lable = [word for word in str(lable)]\n","            lable.insert(0, '<BOS>')\n","            lable.append('<EOS>')\n","            lable = map(lambda x: self.stoi[x], lable)\n","            self.items.append({'path': dicts['path'], 'lable': list(lable)})\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.items[idx]\n"]},{"cell_type":"markdown","metadata":{"id":"pQKaV-aJkyB2"},"source":["Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w87NlTYHbYEe","outputId":"9f554ba8-02eb-4292-ce24-e4c28af9df32","trusted":true},"outputs":[],"source":["!pip install einops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kC8H-c5iT-g","trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","from torch import nn, einsum\n","from einops import rearrange, repeat, reduce\n","from rich import console\n","\n","console = console.Console()\n","device = 'cuda' if torch.device('cuda') else 'cpu'\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, head_num, emb_d):\n","        super(MultiHeadAttention, self).__init__()\n","        self.head_num = head_num\n","        self.emb_d = emb_d\n","        self.dk = (emb_d // head_num) ** (1 / 2)\n","        self.qkv_layer = nn.Linear(emb_d, emb_d * 3, bias=False)\n","        self.out_attention = nn.Linear(emb_d, emb_d, bias=False)\n","\n","    def forward(self, x, mask=None, kv=None):\n","        qkv = self.qkv_layer(x)\n","        q, k, v = tuple(rearrange(qkv, pattern='b t (h k d) -> k b h t d', h=self.head_num, k=3))\n","        if kv is not None:\n","            # decoder 部分\n","            # kk = repeat(kv, 'b t (d)->b t (d h)', h=self.head_num)\n","            k = rearrange(kv, 'b t (h d)-> b h t d', h=self.head_num)\n","            v = k\n","        energy = einsum('... i d , ... j d -> ... i j', q, k)\n","        energy = energy / self.dk\n","        if mask is not None:\n","            # console.log(mask)\n","            energy += mask\n","            # console.log(energy[0][0])\n","        attention = torch.softmax(energy, dim=-1)\n","        # console.log(attention[0][0])\n","        xx = einsum('... i j , ... j d -> ... i d', attention, v)\n","        xx = rearrange(xx, 'b h t d -> b t (h d)')\n","        xx = self.out_attention(xx)\n","        # console.log(xx)\n","        return xx\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkDwZ1MsjmKY","trusted":true},"outputs":[],"source":["import math\n","\n","import torch\n","from torch import nn\n","\n","\n","class PositionalEncoding(nn.Module):\n","    \"\"\"Implement the PE function.\"\"\"\n","\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) *\n","                             -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        # print(self.pe.shape)\n","        # print(x.shape)\n","        x = x + self.pe[:, :x.size(2)]\n","        return self.dropout(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Hoqq1cRjuVu","trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","class EncoderBlock(nn.Module):\n","    def __init__(self, mlp_d, head_num, emb_d, dropout):\n","        super(EncoderBlock, self).__init__()\n","\n","        self.m_attention = MultiHeadAttention(head_num, emb_d)\n","        self.mlp_layer = MLP(emb_d, mlp_d)\n","        self.drop_layer = nn.Dropout(dropout)\n","        self.norm_layer1 = nn.LayerNorm(emb_d)\n","        self.norm_layer2 = nn.LayerNorm(emb_d)\n","\n","    def forward(self, x):\n","        x = self.m_attention(x=x)\n","        _x = self.drop_layer(x)\n","        x = _x + x\n","        x = self.norm_layer1(x)\n","\n","        x = self.mlp_layer(x)\n","        x = _x + x\n","        x = self.norm_layer2(x)\n","        return x\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_d, middle_d):\n","        super(MLP, self).__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(input_d, middle_d),\n","            nn.Dropout(0.1),\n","            nn.GELU(),\n","            nn.Linear(middle_d, input_d),\n","            nn.Dropout(0.1)\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V3JZNmw4kBx4","trusted":true},"outputs":[],"source":["class DecoderBlock(nn.Module):\n","    def __init__(self, mlp_d, emb_d, dropout, head_num):\n","        super(DecoderBlock, self).__init__()\n","        self.masked_attention = MultiHeadAttention(head_num, emb_d)\n","        self.attention = MultiHeadAttention(head_num, emb_d)\n","        self.drop_layer = nn.Dropout(dropout)\n","        self.normal_layer0 = nn.LayerNorm(emb_d)\n","        self.normal_layer2 = nn.LayerNorm(emb_d)\n","        self.normal_layer3 = nn.LayerNorm(emb_d)\n","        self.mlp_layer = MLP(emb_d, mlp_d)\n","\n","    def forward(self, x, kv, mask):\n","        mask_attention = self.masked_attention(x=x, mask=mask)\n","        # console.log(mask_attention)\n","        _x = self.drop_layer(mask_attention)\n","        x = _x + x\n","        x = self.normal_layer0(x)\n","\n","        multihead_attention = self.attention(x=mask_attention, kv=kv)\n","        _x = self.drop_layer(multihead_attention)\n","        x = _x + x\n","        x = self.normal_layer2(x)\n","\n","        _x = self.mlp_layer(x)\n","        x = _x + x\n","        x = self.normal_layer3(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alTghqBhkkbD","trusted":true},"outputs":[],"source":["from rich import console\n","\n","console = console.Console()\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, block_size, mlp_d, head_num, emd_d, dropout):\n","        super(Encoder, self).__init__()\n","        self.layers = nn.ModuleList(\n","            EncoderBlock(mlp_d, head_num, emd_d, dropout) for _ in range(block_size)\n","        )\n","\n","    def forward(self, x):\n","        # b t emb_d\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yuk_E7p9kn9c","trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, block_size, mlp_d, head_num, emb_d, dropout):\n","        super(Decoder, self).__init__()\n","        self.layers = nn.ModuleList(\n","            DecoderBlock(mlp_d, emb_d, dropout, head_num) for _ in range(block_size)\n","        )\n","\n","    def forward(self, x, mask, enc_out):\n","        # b t emb_d\n","        # console.log(x.shape)\n","        for layer in self.layers:\n","            x = layer(x=x, mask=mask, kv=enc_out)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GkVc5WLkqfx","trusted":true},"outputs":[],"source":["import torch\n","from matplotlib import pyplot as plt\n","from torch.nn import TransformerEncoder\n","from torchvision.transforms import transforms\n","from einops import repeat\n","from rich import console\n","\n","console = console.Console()\n","device = 'cuda' if torch.device('cuda') else 'cpu'\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, block_size, mlp_d, head_num, emb_d, dropout, in_channels, img_x, img_y, patch_d, vocab):\n","        super(Transformer, self).__init__()\n","        self.img_w = img_x\n","        self.img_h = img_y\n","        self.patch_d = patch_d\n","        self.in_channels = in_channels\n","        self.token_d = in_channels * (patch_d ** 2)\n","        self.token_num = (img_y // patch_d) * (img_x // patch_d)\n","\n","        self.pos_emb = nn.Parameter(\n","            torch.randint(low=0, high=5, size=(self.token_num, emb_d), device=device).float())\n","        nn.init.xavier_normal_(self.pos_emb)\n","        # self.pos_emb = PositionalEncoding(d_model=emb_d, dropout=dropout, max_len=self.token_num)\n","        # self.cls_emb = nn.Parameter(torch.randn(1, 1, emb_d, device=device))\n","        # nn.init.xavier_normal_(self.pos_emb)\n","        # self.transformer_encoder = nn.TransformerEncoder(num_layers=block_size, norm=True)\n","        self.encoder = Encoder(block_size, mlp_d, head_num, emb_d, dropout)\n","        self.decoder = Decoder(block_size, mlp_d, head_num, emb_d, dropout)\n","\n","        self.emb_layer = nn.Linear(self.token_d, emb_d)\n","        self.embbding_layper = nn.Embedding(vocab, emb_d)\n","        self.drop_layer = nn.Dropout(dropout)\n","\n","        self.dense_layer = nn.Linear(emb_d, vocab)  # self.out_h = nn.LeakyReLU()\n","        self.out_h = nn.GELU()\n","        # self.dense_layer1 = nn.Linear(emb_d // 2, vocab)\n","        # self.sigmod = nn.Tanh()\n","        self.softmax_layer = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, words, enc_out=None, img=None, **kwargs):\n","        if img is not None:\n","            # console.log(img.shape)\n","            # B C H W\n","            img = rearrange(img, 'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n","                            patch_x=self.patch_d,\n","                            patch_y=self.patch_d)\n","            # imm = transforms.ToPILImage()\n","            # plt.imshow(imm(img[0]))\n","            # plt.show()\n","            img = self.emb_layer(img)\n","            # x: [b t emb_d]\n","            batch, tokens, _ = img.shape\n","            # 分类+位置初始化编码\n","            # cls = repeat(self.cls_emb, 'b ...-> (b batches) ...', batches=batch)\n","            # img = torch.cat([cls, img], dim=1)\n","\n","            img += self.pos_emb\n","            # img = self.pos_emb(img)\n","            img = self.drop_layer(img)\n","\n","            enc_out = self.encoder(img)\n","            # enc_out = self.transformer_encoder(img)\n","            # console.log(enc_out[0][-1])\n","        emb = self.embbding_layper(words)\n","\n","        # mask = torch.triu(torch.ones(len(words[0]), len(words[0])), diagonal=1).bool().cuda()\n","        mask = nn.Transformer.generate_square_subsequent_mask(len(words[0]), device)\n","        # console.log(mask)\n","        next_word = self.decoder(x=emb, enc_out=enc_out, mask=mask)\n","        # console.log(next_word[0][-1])\n","        # console.log(next_word.shape)\n","        out = self.dense_layer(next_word)\n","        # out = self.out_h(out)\n","        # out:[b t d]\n","        # out = self.out_h(out)\n","        # out = self.dense_layer1(out)\n","        # out = self.sigmod(out)\n","        # out = self.softmax_layer(out)\n","        # console.log(out[0])\n","        # console.log(torch.argmax(out[0], dim=-1))\n","\n","        return out, enc_out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"]},{"cell_type":"markdown","metadata":{"id":"0i2cla1Dmvwn"},"source":["Test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":781},"id":"tGVSSc2cmw3Q","outputId":"19eac727-c535-4ebc-c14b-23069cf73a7a","scrolled":true,"trusted":true},"outputs":[],"source":["# @title 默认标题文本\n","import random\n","\n","import PIL.ImageShow\n","from PIL import Image\n","import torch\n","from torch import optim\n","from torchvision import transforms\n","from torch.utils.data import random_split\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from rich import console\n","console =console.Console()\n","\n","trans = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.ConvertImageDtype(torch.float),\n","])\n","\n","cab = get_default_vocab()\n","\n","\n","def main():\n","    datas = MyDataSet(r'./captcha.csv', cab)\n","    # console.log(cab.get_stoi())\n","    train_size = int(len(datas) * 0.95)\n","    test_size = len(datas) - train_size\n","    train_dataset, test_dataset = random_split(datas, [train_size, test_size])\n","    model = train(train_dataset, test_dataset)\n","    torch.save(model,r'./aa.pt')\n","\n","\n","\n","def train(train_set, test_set):\n","    epochs = 500\n","    dataloader = DataLoader(train_set, batch_size=60, shuffle=False, drop_last=True, collate_fn=col_fn,\n","                            num_workers=0)\n","    # for batch in dataloader:\n","    #     console.log(batch['lables'].shape)\n","    #     console.log(batch['imgs'].shape)\n","    model = Transformer(block_size=4, mlp_d=195, head_num=5, emb_d=180, dropout=0.1, in_channels=1, img_x=60,\n","                        img_y=160,\n","                        patch_d=4,\n","                        vocab=38).to(device)\n","    print(torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model)\n","    model = model.cuda()\n","\n","    ce_loss = nn.CrossEntropyLoss()\n","    opti = optim.Adam(model.parameters(), lr=0.0007)\n","    for epoch in tqdm(range(epochs), leave=False):\n","        loss_epoch = 0\n","        for batch in dataloader:\n","            # { lables:[b t],imgs:[b c w h]}\n","            # console.log(len(batch['lables']))\n","            # console.log(batch['imgs'].shape)\n","            # console.log(batch['lables'].shape)\n","            decoder_input, decoder_tgt = get_decoder_tgt_and_input(batch)\n","            # console.log(decoder_input[:5])\n","            # console.log(decoder_tgt[:5])\n","            # inp:[b t];img:[b c h w]\n","\n","            # imm = transforms.ToPILImage()\n","            # plt.imshow(imm(batch['imgs'][0]))\n","            # plt.show()\n","            # console.log(batch['imgs'].shape)\n","            # console.log(get_text_from_vocab(decoder_input[0]))\n","            # console.log(f\"path:{batch['path'][0]};text:{get_text_from_vocab(batch['lables'][0])}\")\n","\n","            out, _ = model(img=batch['imgs'].cuda(), words=decoder_input)\n","            # out, _ = model(img=torch.randn(batch['imgs'].shape).cuda(),\n","            #                words=torch.randint(size=decoder_input.shape, low=1, high=39).long().cuda())\n","            # console.log(pred.shape)\n","            # console.log(out)\n","            out = rearrange(out, 'b t d-> (b t) d')\n","            decoder_tgt = rearrange(decoder_tgt, 'b t->(b t)')\n","            # console.log(f'inp:{torch.randint(size=decoder_input.shape, low=1, high=39).long().cuda()[1]}')\n","            # console.log(f'out:{torch.argmax(out, dim=-1)[:10]}')\n","            # console.log(f'tgt:{decoder_tgt[:10]}')\n","            loss = ce_loss(input=out, target=decoder_tgt)\n","            # console.log(decoder_tgt[4].item())\n","            # console.log(out[4][decoder_tgt[4].item()])\n","            opti.zero_grad()\n","            loss.backward()\n","            opti.step()\n","            loss_epoch += loss\n","\n","            # print(loss)\n","        model.eval()\n","        test_loss = evaluate(model, test_set=test_set)\n","        model.train()\n","        # tqdm.set_description(f'Epoch {epoch + 1}, Loss {loss_epoch / (len(train_set) / 50):.4f}', refresh=True)\n","        console.log(f'epoch: {epoch + 1};loss: {loss_epoch / (len(train_set) / 50):.4f};test_acc:{test_loss:.4f}')\n","    return model\n","\n","\n","def evaluate(model, test_set):\n","    test_set_loader = DataLoader(test_set, batch_size=50, shuffle=False, drop_last=True, collate_fn=col_fn,\n","                                 num_workers=0)\n","\n","    # model.eval()\n","    acc_count = []\n","    out_sample = []\n","    tgt_sample = []\n","    for batch in tqdm(test_set_loader, leave=False, dynamic_ncols=True):\n","        decoder_input, decoder_tgt = get_decoder_tgt_and_input(batch)\n","        out, kv = model(imgs=batch['imgs'], words=decoder_input)\n","        # out:[b t d],kv:[b t d]\n","\n","        decoder_tgt = rearrange(decoder_tgt, 'b t->(b t)')\n","        out = rearrange(out, 'b t d-> (b t) d')\n","\n","        out = torch.argmax(out, dim=-1)\n","\n","        equal = torch.eq(out, decoder_tgt)\n","        # console.log(f'inp:{rearrange(decoder_input, \"b t->(b t)\")[:5]}')\n","        out_sample.append(out[:6])\n","        tgt_sample.append(decoder_tgt[:6])\n","        # 计算相等元素的个数，返回一个标量\n","        count = torch.sum(equal) / len(out)\n","        acc_count.append(count)\n","    console.log(f'out:{random.choice(out_sample)}')\n","    console.log(f'tgt:{random.choice(tgt_sample)}')\n","    return torch.mean(torch.Tensor(acc_count))\n","\n","\n","# def test_model(model: nn.Module):\n","\n","\n","def get_decoder_tgt_and_input(batch):\n","    decoder_input = batch['lables'].cuda()[:, :-1]\n","    decoder_tgt = batch['lables'].cuda()[:, 1:]\n","    return decoder_input, decoder_tgt\n","\n","\n","def col_fn(batch):\n","    imgs = []\n","    lables = []\n","    paths = []\n","    for one_data in batch:\n","        img = Image.open(one_data['path']).convert('L')\n","        # img = Image.open(one_data['path'])\n","        # console.log(f\"path:{one_data['path']};text:{get_text_from_vocab(one_data['lable'])}\")\n","        imgs.append(trans(img))\n","        # img.show()\n","        lables.append(one_data['lable'])\n","        paths.append(one_data['path'])\n","        # console.log(one_data['lable'])\n","    return {'imgs': torch.stack([tensor for tensor in imgs]), 'lables': torch.Tensor(lables).long(), 'path': paths}\n","\n","\n","def get_text_from_vocab(lable):\n","    mapp = cab.get_itos()\n","    return [mapp[idx] for idx in lable]\n","\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
